import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd

# -----------------------------
# CONFIG ‚Äì seznam sledovan√Ωch URL
# -----------------------------
URLS = [
    "https://www.o2.cz/",
    "https://www.o2.cz/osobni/o2spolu",
    "https://www.o2.cz/osobni/oneplay"
]

# -----------------------------
# HELPERS
# -----------------------------
def fetch_html(url):
    """St√°hne HTML str√°nky."""
    try:
        r = requests.get(url, timeout=20)
        return r.text
    except:
        return ""


def fetch_pagespeed(url, strategy):
    """
    Z√≠sk√° performance sk√≥re (0-100) z PageSpeed Insights bez API kl√≠ƒçe.
    strategy: "mobile" nebo "desktop"
    Vrac√≠ int (0-100).
    """
    base = "https://www.googleapis.com/pagespeedonline/v5/runPagespeed"
    params = {
        "url": url,
        "strategy": strategy,
        "category": "performance"
    }
    try:
        r = requests.get(base, params=params, timeout=30)
        r.raise_for_status()
        data = r.json()
        # cesta v JSONu: lighthouseResult.categories.performance.score (0..1 float)
        score = data.get("lighthouseResult", {}) \
                    .get("categories", {}) \
                    .get("performance", {}) \
                    .get("score", None)
        if score is None:
            print(f"[PSI] No performance score for {url} ({strategy}) - JSON keys missing")
            return 0
        # score je float 0..1, p≈ôevedeme na 0..100 int
        try:
            return int(float(score) * 100)
        except Exception:
            return 0
    except Exception as e:
        # vypi≈° d≈Øvod do logu (uvid√≠≈° to v Actions)
        print(f"[PSI] Error fetching PSI for {url} ({strategy}): {e}")
        return 0


def measure_seo(soup):
    """SEO sk√≥re."""
    score = 100
    issues = []

    if not soup.find("title"):
        score -= 10
        issues.append("Chyb√≠ <title>")

    if not soup.find("meta", attrs={"name": "description"}):
        score -= 10
        issues.append("Chyb√≠ meta description")

    h1 = soup.find_all("h1")
    if len(h1) == 0:
        score -= 10
        issues.append("Chyb√≠ H1")
    if len(h1) > 1:
        score -= 5
        issues.append("V√≠ce ne≈æ jedno H1")

    return max(10, score), issues


def measure_ai_score(soup):
    """AI/LLM sk√≥re."""
    score = 100
    issues = []

    text = soup.get_text(" ", strip=True)
    words = len(text.split())

    if words < 200:
        score -= 20
        issues.append("M√°lo textu")
    elif words < 500:
        score -= 10

    if len(soup.find_all("h2")) < 2:
        score -= 10
        issues.append("M√°lo H2 nadpis≈Ø")

    return max(10, score), issues


def analyze_images(soup):
    """Najde obr√°zky s probl√©my."""
    problems = []
    for img in soup.find_all("img"):
        src = img.get("src") or ""
        alt = img.get("alt")

        if not src.startswith("http"):
            continue

        if not src.lower().endswith(".webp"):
            problems.append(("Non-WEBP", src, "Obr√°zek nen√≠ ve form√°tu WEBP"))

        if not alt or alt.strip() == "":
            problems.append(("Chyb√≠ ALT", src, "Chybƒõj√≠c√≠ nebo pr√°zdn√Ω ALT"))

    return problems

# -----------------------------
# P≈ò√çPRAVA SLO≈ΩEK A CSV
# -----------------------------
os.makedirs("reports", exist_ok=True)
os.makedirs("data", exist_ok=True)

csv_path = "data/metrics.csv"

if not os.path.exists(csv_path):
    with open(csv_path, "w", encoding="utf-8") as f:
        f.write("date,url,mobile_perf,desktop_perf,seo,ai\n")

today = datetime.now().strftime("%d.%m.%Y")
report_path = f"reports/report_{today}.md"

rows = []
sections = []

# -----------------------------
# HLAVN√ç LOGIKA ANAL√ùZY
# -----------------------------
for url in URLS:

    # PSI performance
    mobile_perf = fetch_pagespeed(url, "mobile")
    desktop_perf = fetch_pagespeed(url, "desktop")

    # HTML anal√Ωza
    html = fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")
    seo, seo_issues = measure_seo(soup)
    ai, ai_issues = measure_ai_score(soup)
    img_problems = analyze_images(soup)

    # Ulo≈æen√≠ do CSV
    rows.append([today, url, mobile_perf, desktop_perf, seo, ai])

    # -----------------------------
    # SEKCION√ÅLN√ç REPORT
    # -----------------------------
    s = f"## üîµ {url}\n\n"
    s += f"### üì± Mobile Performance: **{mobile_perf}**\n"
    s += f"### üñ• Desktop Performance: **{desktop_perf}**\n"
    s += f"### üîç SEO: **{seo}**\n"
    s += f"### ü§ñ AI/LLM: **{ai}**\n\n"

    if seo_issues or ai_issues:
        s += "### üö® Zji≈°tƒõn√© probl√©my:\n"
        for i in seo_issues: s += f"- SEO: {i}\n"
        for i in ai_issues: s += f"- AI: {i}\n"
        s += "\n"

    if img_problems:
        s += "### üñº Probl√©my s obr√°zky\n\n"
        s += "| Typ | URL | Detail |\n|-----|-----|--------|\n"
        for typ, src, detail in img_problems:
            s += f"| {typ} | {src} | {detail} |\n"
        s += "\n"
    else:
        s += "### ‚úî Obr√°zky jsou v po≈ô√°dku\n\n"

    sections.append(s)

# -----------------------------
# DOPLNƒöN√ç CSV
# -----------------------------
with open(csv_path, "a", encoding="utf-8") as f:
    for r in rows:
        f.write(",".join(map(str, r)) + "\n")

# -----------------------------
# TVORBA GRAF≈Æ
# -----------------------------
# -----------------------------
# ‚ú¶ BEZPEƒåN√â NAƒåTEN√ç, ƒåI≈†TƒöN√ç A NORMALIZACE CSV
# -----------------------------
import csv

def load_and_clean_csv(path):
    # p≈ôeƒçteme surovƒõ (abychom vidƒõli i ≈°patn√© ≈ô√°dky)
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for r in reader:
            # strip ka≈æd√© polo≈æky
            rows.append([c.strip() for c in r])

    if not rows or len(rows) == 1:
        # ≈æ√°dn√° data kromƒõ hlaviƒçky nebo pr√°zdn√Ω soubor -> vr√°t√≠me pr√°zdn√Ω DF se sloupci, kter√© oƒçek√°v√°me
        expected_cols = ["date","url","mobile_perf","desktop_perf","seo","ai"]
        return pd.DataFrame(columns=expected_cols)

    header = rows[0]
    good_rows = []
    for r in rows[1:]:
        # p≈ôijmeme pouze ≈ô√°dky se stejn√Ωm poƒçtem pol√≠ jako hlaviƒçka a bez √∫plnƒõ pr√°zdn√Ωch (v≈°e pr√°zdn√©)
        if len(r) == len(header) and any(cell != "" for cell in r):
            good_rows.append(r)

    # vytvo≈ô√≠me DF podle hlaviƒçky, pokud je hlaviƒçka neƒçekan√°, dopln√≠me chybƒõj√≠c√≠ sloupce
    df = pd.DataFrame(good_rows, columns=header)

    # oƒçist√≠me duplicity a whitespace
    df = df.drop_duplicates().reset_index(drop=True)

    # standardizujeme n√°zvy sloupc≈Ø (mal√° p√≠smena)
    df.columns = [c.strip() for c in df.columns]

    # zajist√≠me oƒçek√°van√© sloupce (pokud chyb√≠, dopln√≠me s nulami)
    expected = ["date","url","mobile_perf","desktop_perf","seo","ai"]
    for col in expected:
        if col not in df.columns:
            df[col] = 0

    # p≈ôetypujeme ƒç√≠seln√© sloupce na int (chybn√© hodnoty -> 0)
    for col in ["mobile_perf","desktop_perf","seo","ai"]:
        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)

    # oƒçist√≠me date sloupec (udr≈æujeme ve form√°tu DD.MM.YYYY) ‚Äî odstran√≠me whitespace
    df["date"] = df["date"].astype(str).str.strip()

    # p≈ôep√≠≈°eme CSV (ƒçist√° verze) zpƒõt (zachov√°me po≈ôad√≠ expected sloupc≈Ø)
    df_to_write = df[expected]
    df_to_write.to_csv(path, index=False, encoding="utf-8")

    return df_to_write

# naƒçti a vyƒçisti CSV
df = load_and_clean_csv(csv_path)

# pokud je df pr√°zdn√Ω (≈æ√°dn√° data), vytvo≈ô√≠me pr√°zdn√© struktury pro grafy
if df.empty:
    # vytvo≈ô√≠me pr√°zdn√© DataFrame se spr√°vn√Ωmi sloupci, aby zbytek k√≥du nepadl
    df = pd.DataFrame(columns=["date","url","mobile_perf","desktop_perf","seo","ai"])


# --- Graf 1: Performance ---
perf = df.groupby("date")[["mobile_perf", "desktop_perf"]].mean()

plt.figure(figsize=(10, 5))
plt.plot(perf.index, perf["mobile_perf"], label="Mobile")
plt.plot(perf.index, perf["desktop_perf"], label="Desktop")
plt.title("V√Ωvoj Performance")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("reports/performance_trend.png")
plt.close()

# --- Graf 2: SEO + AI ---
seo_ai = df.groupby("date")[["seo", "ai"]].mean()

plt.figure(figsize=(10, 5))
plt.plot(seo_ai.index, seo_ai["seo"], label="SEO")
plt.plot(seo_ai.index, seo_ai["ai"], label="AI/LLM")
plt.title("V√Ωvoj SEO a AI/LLM sk√≥re")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("reports/seo_ai_trend.png")
plt.close()

# -----------------------------
# TVORBA REPORTU
# -----------------------------
with open(report_path, "w", encoding="utf-8") as f:
    f.write(f"# T√Ωdenn√≠ report ‚Äì {today}\n\n")

    f.write("## üìà V√Ωvoj Performance\n")
    f.write("![Performance](../reports/performance_trend.png)\n\n")
    
    f.write("## üìò V√Ωvoj SEO + AI/LLM sk√≥re\n")
    f.write("![SEO AI](../reports/seo_ai_trend.png)\n\n")
    
    f.writelines(sections)

print("Report hotov√Ω:", report_path)
print("CSV aktualizov√°no:", csv_path)
